# src/chatbot_ui.py (Phi√™n b·∫£n cu·ªëi c√πng)
import streamlit as st
import os
from langchain_core.messages import HumanMessage, AIMessage
from langchain_core.output_parsers import StrOutputParser
from langchain_google_genai import ChatGoogleGenerativeAI, HarmBlockThreshold, HarmCategory
from src.database import save_message_to_db, load_messages_from_db
from src.rag_components import format_chat_history

def display_chatbot_interface(vector_stores, prompt, supabase_client, user_id, user_name):
    """Hi·ªÉn th·ªã to√†n b·ªô giao di·ªán chatbot sau khi ƒëƒÉng nh·∫≠p."""

    with st.sidebar:
        # L·∫•y t√™n ng∆∞·ªùi d√πng t·ª´ session_state
        st.write(f'Ch√†o m·ª´ng *{user_name}*')
        if st.button("ƒêƒÉng xu·∫•t"):
            supabase_client.auth.sign_out()
            del st.session_state['user_session']  # Xo√° session ng∆∞·ªùi d√πng
            st.rerun()  # Ch·∫°y l·∫°i script ƒë·ªÉ hi·ªÉn th·ªã form ƒëƒÉng nh·∫≠p
        # L·∫•y authenticator t·ª´ session_state ƒë·ªÉ g·ªçi h√†m logout
        #st.session_state.authenticator.logout('ƒêƒÉng xu·∫•t', 'main')

        st.divider()
        st.title("‚öôÔ∏è T√πy ch·ªçn")
        domain = st.radio("B·ªô ki·∫øn th·ª©c", ["CS50", "L·ªãch s·ª≠"], horizontal=True)
        temperature = st.slider("üå°Ô∏è ƒê·ªô s√°ng t·∫°o", 0.0, 1.0, 0.1, 0.05)
        k_documents = st.slider("üìö S·ªë ngu·ªìn tham kh·∫£o", 3, 10, 5, 1)

    st.title("ü§ñ Tr·ª£ l√Ω h·ªçc t·∫≠p (CS50 & L·ªãch s·ª≠)")

    # T·∫£i l·ªãch s·ª≠ chat c·ªßa ng∆∞·ªùi d√πng n√†y
    if "chat_history" not in st.session_state:
        st.session_state.chat_history = load_messages_from_db(supabase_client, user_id)

    # Hi·ªÉn th·ªã c√°c tin nh·∫Øn ƒë√£ c√≥
    for message in st.session_state.chat_history:
        role = "user" if isinstance(message, HumanMessage) else "assistant"
        with st.chat_message(role):
            st.markdown(message.content)

    # X·ª≠ l√Ω input m·ªõi
    placeholder_text = "H·ªèi t√¥i v·ªÅ CS50 ho·∫∑c L·ªãch s·ª≠ Vi·ªát Nam..."
    if user_question := st.chat_input(placeholder_text):
        st.session_state.chat_history.append(HumanMessage(content=user_question))
        save_message_to_db(supabase_client, user_id, "user", user_question)

        with st.chat_message("user"):
            st.markdown(user_question)

        with st.chat_message("assistant"):
            response_placeholder = st.empty()
            sources_placeholder = st.empty()

            with st.spinner("Tr·ª£ gi·∫£ng ƒëang suy nghƒ©..."):
                llm = ChatGoogleGenerativeAI(
                    model="gemini-2.0-flash",
                    temperature=temperature,
                    # safety_settings={
                    #     HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,
                    #     HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,
                    #     HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,
                    #     HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,
                    # }
                )
                vector_store = vector_stores[domain]
                retriever = vector_store.as_retriever(search_type="mmr", search_kwargs={'k': 4, 'fetch_k': 20})
                
                formatted_chat_history = format_chat_history(st.session_state.chat_history)
                retrieved_docs = retriever.invoke(user_question)
                print("\n--- K·∫æT QU·∫¢ T√åM KI·∫æM (RETRIEVED DOCS) ---")
                if not retrieved_docs:
                    print("!!! C·∫¢NH B√ÅO: Retriever ƒë√£ tr·∫£ v·ªÅ m·ªôt danh s√°ch r·ªóng. Kh√¥ng t√¨m th·∫•y t√†i li·ªáu n√†o.")
                else:
                    print(f"ƒê√£ t√¨m th·∫•y {len(retrieved_docs)} t√†i li·ªáu li√™n quan.")
                    for i, doc in enumerate(retrieved_docs):
                        print(f"\n--- T√†i li·ªáu {i+1} ---")
                        # In m·ªôt ph·∫ßn n·ªôi dung ƒë·ªÉ xem tr∆∞·ªõc
                        print(doc.page_content[:500] + "...") 
                        # In metadata ƒë·ªÉ bi·∫øt n√≥ ƒë·∫øn t·ª´ file n√†o
                        print(f"Metadata: {doc.metadata}") 
                print("=============================================\n")
                
                
                context = "\n\n---\n\n".join([doc.page_content for doc in retrieved_docs])

                rag_chain = prompt | llm | StrOutputParser()
                response_stream = rag_chain.stream({
                    "domain": domain,
                    "question": user_question,
                    "context": context,
                    "chat_history": formatted_chat_history
                })

                # Tr√°nh ph·ª• thu·ªôc write_stream (c√≥ th·ªÉ g√¢y import pyarrow tr√™n Windows)
                accumulated_parts = []
                for chunk in response_stream:
                    accumulated_parts.append(chunk)
                    response_placeholder.markdown("".join(accumulated_parts))
                full_response = "".join(accumulated_parts)

            # Hi·ªÉn th·ªã ngu·ªìn sau khi stream xong
            with sources_placeholder.container():
                with st.expander("Xem c√°c ngu·ªìn tham kh·∫£o"):
                    if retrieved_docs:
                        for doc in retrieved_docs:
                            full_path = doc.metadata.get('source', 'Kh√¥ng r√µ ngu·ªìn')
                            source_name = os.path.basename(full_path)
                            st.markdown(f"**Ngu·ªìn:** `{source_name}`")
                    else:
                        st.write("Kh√¥ng t√¨m th·∫•y t√†i li·ªáu li√™n quan.")

        st.session_state.chat_history.append(AIMessage(content=full_response))
        save_message_to_db(supabase_client, user_id, "assistant", full_response)